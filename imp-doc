MATPROD - A LIBRARY FOR MATRIX MULTIPLICATION WITH OPTIONAL PIPELINING
          Implementation Documentation 

Copyright (C) 2017, 2018 Radford M. Neal.

  The matprod library is free software; you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation; either version 2 of the License, or
  (at your option) any later version.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License along
  with this program; if not, write to the Free Software Foundation, Inc.,
  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.


The routines in the matprod library have been written to be reasonably
fast.  However, compared to the matrix multiply routines in optimized
BLAS libraries (DGEMM, DGEMV, and DDOT), the speed of the matprod
routines is limited by the requirement that they produce the same
results as the naive algorithms, by the requirement that no
substantial temporary storage be used, and by the lesser amount of
effort that has been expended in trying to make them fast.  The
general implementation strategy for matprod is described here, along
with some details of the implementation.


General algorithmic approaches.

This section describes the general algorithms used when computations
are done in a single thread, the matrices are small enough that the
computation is not split into parts to improve cache performance, and
special case code for small values of n or k does not apply.

VEC_VEC... The matprod_vec_vec procedure uses what is essentially the
naive algorithm, successively adding products of elements, since any
reordering of additions could change the result.

MAT_MAT... When matprod_mat_mat multiplies matrices x and y to produce
result z, it computes columns of z in succession.  To compute a column
of z, the column is first set to the sum of the products of the first
two columns of x and the first two elements of the column of y
corresponding that of z.  Then, the product of another column of x
with another element of y is added to this column of z - this being
done for two columns of x together (unless only one column remains),
in order to reduce accesses to elements of z.  Two columns of z are
computed concurrently by this method (unless only one remains) so that
the data fetched from the columns of x can be used twice.  When this
approach is applied to a small matrix, two columns of z will fit in
the L1 data cache, and so will be quickly accessible when products
with columns of x are added repeatedly to it (even if not all of x, y,
and z fit in the L1 cache).  Note that this should generally be faster
than the naive method of computing each element of the result as a dot
product of a row and column, since accesses to the elements of the row
will be non-sequential.

OUTER... For matprod_outer, there is only one column of x, which is
multiplied by successive elements of y to give successive columns of
z.  This is presently done one column at a time.  (It is not clear
that doing two columns concurrently would be faster.)

MAT_VEC... The matprod_mat_vec procedure uses the same method as
matprod_mat_mat, with y having only a single column.

VEC_MAT... A different approach is used for matprod_vec_mat, for which
columns of x and z have only one element.  Successive elements of the
result are computed as dot products of x with successive columns of y.
To allow use of SIMD instructions (or other instruction-level
parallelism), and to reuse data fetched from x, four such dot products
are computed concurrently (except when fewer than four columns of y
remain).

TRANS1... The method for matprod_trans1 is similar to that for
matprod_vec_mat, except that eight dot products are computed
concurrently for four columns of x and two columns of y, which produce
eight elements of z in a four-by-two block.

TRANS2... The method for matprod_trans2 is similar to that for
matprod_mat_mat, except that the elements multiplying columns of x are
taken from rows of y (rather than columns of y).

TRANS12... The implementation of matprod_trans12 is based on the fact
that the product of the transposes of two matrices is the transpose of
the product of these matrices with operands swapped.  The same
procedures used for matprod_mat_mat are used here, storing the result
in a local variable (on the stack), the transpose of which is then
copied to z.  For large matrices, this is done repeatedly for portions
of z, in order to limit the amount of stack space used to no more than
TRANS12_ZELEM (currently 2048) doubles (each 8 bytes in size), both
for cache performance reasons and because stack space may be limited
in some environments.


Special cases.

Special code is used to handle some cases in which n, k, or m is small
(for multiplication of an n x k matrix by a k x m matrix).  This can
reduce overhead when what would otherwise be the innermost loop would
be done only a small number of times.  Other special tricks may also
be possible - for example, when columns have only two elements, AVX
instructions can operate on all elements in two columns at once.

The scalar-vector, vector-vector, matrix-vector, vector-matrix, and
outer product procedures are particular cases of such special-case
optimization that are made visible in the API, but other special cases
are handled by non-visible functions (including special cases of the
matrix-vector, vector-matrix, and outer product operations).


Optimizing for cache behaviour.

The code is written assuming that the processor has an L1 data cache
of 32K bytes (or more) and an L2 cache of 256K bytes (or more).  The
size of a cache line is assumed to be 64 bytes.  If these assumptions
are not true, the code will of course still work correctly, but
perhaps more slowly than it might with some other assumptions.  Note
that 4K 8-byte double values fit in the L1 data cache, but it is best
to rely only on somewhat fewer fitting, since the L1 cache will
contain some other values (eg, top entries in the stack) as well.

A last-level cache holding at least DOUBLES_IN_LLC doubles (currently
100000) is also assumed to possibly exist (though perhaps be the same
as the L2 cache).  If such a large last-level cache does not exist,
there is only a small penalty for using the code that assumes it does
exist.

VEC_VEC... For matprod_vec_vec, there is no apparent way of improving
on the obvious method that accesses memory sequentially.

VEC_MAT... For matprod_vec_mat, if y has more than VEC_MAT_YROWS rows,
the operation is done on successive parts of the matrix, each
consisting of at most VEC_MAT_YROWS of the rows.  The second and later
parts add to the result found by earlier parts.  The value for
VEC_MAT_YROWS, currently 512, is designed to keep the vector x in the
L1 cache, given that x and four columns of y (all of length
VEC_MAT_YROWS) are accessed within the main loop.

MAT_VEC... For matprod_mat_vec, if x has more than MAT_VEC_XROWS rows,
the operation is done on successive parts of the matrix, each
consisting of at most MAT_VEC_XROWS of the rows.  The definition of
MAT_VEC_XROWS, currently 1024, is designed to keep the vector z in the
L1 cache, given that z and two columns of x (all of length
MAT_VEC_XROWS) are accessed within the main loop.

OUTER... For matprod_outer, if x has more than OUTER_ROWS elements
(and hence z has more than OUTER_ROWS rows), the operation is done on
successive parts of x, each consisting of at most OUTER_ROWS elements.
The definition of OUTER_ROWS, currently 2048-128, is designed to keep
that part of x in the L1 cache, given that it and the corresponding
part of a column of z are accessed within the main loop.

MAT_MAT... For matprod_mat_mat, if x has more than MAT_MAT_XROWS rows,
the operation is done on successive groups of rows of the matrix, each
consisting of at most MAT_MAT_XROWS of the rows.  Furthermore,
operations on each set of up to MAT_MAT_XROWS may be split into parts
by columns of x, with all except for the first of these split
operations adding to the sum from the operations on previous columns.

The definition of MAT_MAT_XROWS, currently 1024-64, is designed to
keep two columns of z in the L1 cache, given that two columns of z and
two columns of x (all of length MAT_MAT_XROWS) are accessed within the
outer loop over columns of y and z.

The splitting according to columns of x is designed to keep the
sub-matrix of x that is multiplied repeatedly by columns of y in the
L2 cache.  If the sub-matrix has the maximum MAT_MAT_XROWS, the number
of columns will be MAT_MAT_XCOLS, currently 32.  If the number of rows
is less than MAT_MAT_XROWS, the number of columns may be larger, while
limiting the number of elements to MAT_MAT_XCOLS*MAT_MAT_XROWS.

If y is large, and rows of x will be split, matprod_mat_mat accesses
columns of y, and computes corresponding columns of z, in groups,
whose size is designed to keep these columns of y and z in a
last-level cache that can hold DOUBLES_IN_LLC doubles.

TRANS1... For matprod_trans1, if x has more than TRANS1_XROWS rows,
the operation is done on successive groups of rows of the matrix, each
consisting of at most MAT_MAT_XROWS of the rows.  For all but the
first of these groups of rows, the products of elements of x and y are
added to the sums from the previous groups of row.  Furthermore,
operations on each set of up to MAT_MAT_XROWS may be split into parts
by columns of x.

The definition of TRANS1_XROWS, currently 512, is designed to keep two
columns of y in the L1 cache, given that two columns of y and four
columns of x (all of length TRANS1_XROWS) are accessed within the loop
computing (or adding to) elements of z.

The splitting according to columns of x is designed to keep the
sub-matrix of x that is multiplied repeatedly by columns of y in the
L2 cache.  If the sub-matrix has the maximum TRANS1_XROWS, the number
of columns will be TRANS1_XCOLS, currently 48.  If the number of rows
is less than TRANS1_XROWS, the number of columns may be larger, while
limiting the number of elements to TRANS1_XCOLS*TRANS1_XROWS.

As for matprod_mat_mat, if y is large, and rows of x will be split,
matprod_trans1 accesses columns of y, and computes corresponding
columns of z, in groups, whose size is designed to keep these columns
of y and z in a last-level cache that can hold DOUBLES_IN_LLC doubles.

TRANS2... For matprod_trans2, sub-matrices of x are operated on
analogously to matprod_mat_mat, with the definitions of TRANS2_XROWS
and TRANS2_XCOLS currently being the same as those of MAT_MAT_XROWS
and MAT_MAT_XCOLS.  If x is split by row, columns of z are similarly
computed in groups, which however correspond to groups of rows of y,
rather than columns, with the aim of keeping these parts of y and z in
a last-level cache.


Symmetric cases for trans1 and trans2.

When the operands for the trans1 or trans2 operations are identical
(same pointers, and same dimensions), the result is a symmetrical
matrix, only half (including diagonal) of which needs to be computed,
with the remainder being copied from the other half.


SIMD instructions and alignment.

For Intel/AMD processors, the matprod routines can be compiled so that
some code sections use the SSE2, SSE3, AVX, and AVX2 instrinsics
provided by gcc.  Other compilers may also provide such intrinsics;
their availability is assumed to be given by whether the symbols
__SSE2__, __SSE3__, __AVX__, or __AVX2__ are defined.  For all
operations, code written in standard C is provided, for use on
non-Intel/AMD processors, or when the intrinsics are not available, or
when use of the intrinsics is disabled.  There are often code sections
for more than one of __SSE2__, __SSE3__, etc, with the most
powerful/recent option used when the routine is compiled for a
processor for which it is available.  Sometimes only SSE2 or SSE3 code
is provided, because there is no advantage to using AVX, or only AVX
code is provided, because trying to do it with SSE2 or SSE3 does not
work well.  (Or, of course, suitable AVX or SSE2/SSE3 code may just
not have been written yet.)

Optional specification of alignment of arguments is allowed (ie, of
the element of the matrix in the first row and column).  Both the
alignment boundary (8, 16, or 32 bytes) and a possible offset from
this boundary (a multiple of 8) may be specified.  

Some code may be generated conditionally on the alignment, for example
to do some operations before a loop so that the loop operations are
aligned at good boundaries.  For gcc and gcc look-alikes, alignment is
sometimes declared using the __builtin_assume_aligned function
provided in gcc, which may allow the compiler to generate better code.
In particular, gcc will convert some SSE or AVX intrinsics that
specify unaligned accesses to ones specifying aligned acccesses if it
knows the operand always aligned.  Nevertheless, selection of aligned
versus unaligned operations is often done explicitly in the code (via
some macros that are defined, such as _mm_loadA_pd).

Sometimes, rows or columns are stepped through by multiples of 4,
since this preserves whatever aligment (up to 32 bytes) and offset
were present for the first element of the matrix.

In some places, alignment is checked at run-time - for example, when
aligment of the start of a matrix column depends on whether the number
of rows in the matrix is even or odd.  

It is intended that even if no explicit SIMD instructions are used,
and no alignment information is specified, the matprod routines should
still be substantially faster than a naive implementation.


Parallel operation and pipelining.

For most of the procedures, parallelization is done by splitting the
operation to be done according to columns of the results matrix, z,
with the first task scheduled doing the first group of columns, and so
forth.  Except for the trans2 and trans12 operations, this also
corresponds to splitting according to columns of the second operand.
Except for trans12, operations parallelized by columns of the result
also pipeline the output by column (but do not necessarily make data
available immediately after every column is computed).

To allow the parallel/pipelined routines to access the non-visible
procedures in matprod.c for operating on parts of matrices, the
piped-matprod.c source file includes the matprod.c source file.  This
also allows the apparatus for output pipelining to be implemented in
the procedures in matprod.c using macros that are defined to do
nothing when matprod.c is compiled separately - eliminating any
overhead for pipelining when it is not used.  When included in
piped-matprod.c, the normally-visible procedures in maptrod.c are made
"static", allowing them to co-exist with procedures of the same name
obtained by compiling matprod.c separately (which will not have the
pipelining code, and could perhaps have other alignment settings).

VEC_VEC... For the vec_vec operation, parallelization of the additions
is not possible, if the round-off behaviour of simple sequential
summation is to be preserved.  Code that attempts to parallelize
fetches from the first operand is present, but disabled since it seems
not to help.

MAT_VEC... For the mat_vec operation, the second operand is a single
column, so parallelization is done over groups of rows of the first
operand, which correspond to groups of elements of the result,
allowing pipelining for the result.  The group boundaries are adjusted
to correspond to cache lines, to avoid slowdowns due to "false
sharing" in the result vector when two threads store into the same
cache line.


Avoidance of overflow.

The numbers of rows and columns in a matrix have the C "int" type,
which is typically 32 bits in size, allowing a maximum of 2^31-1 for
the number of rows or columns.  However, the number of elements in a
matrix (the product of the number of rows and the number of columns)
may be greater than 2^32-1.  Care is therefore required to avoid
overflow when accessing an element.

If an index into a matrix is computed from a row and column index, it
is necessary to cast the "int" values to "size_t" values - for
example, if x is a matrix with n rows, and i and j are row and column
indexes, element i,j should be accessed as x[i+(size_t)j*n], not as
x[i+j*n].  This form of access is usually avoided, however, in favour
of accesses using pointers into the matrix.  For example, if p points
to the first element in a column of x, then p[i] can be used to access
the element of that column in row i.  The element in row i of the next
column can be accessed as (p+n)[i], which should not be rewritten as
p[n+i], since n+i might overflow.

Note also that p += 2*n is not safe, but p += n; p += n; will be safe.
Similarly, p + 2*n is unsafe, but p + n + n, which means the same as
(p + n) + n, is safe.

Since the result of overflow with signed arithmetic is undefined in C,
it is possible that the unsafe code will actually work when overflow
happens.  But of course this cannot be relied on.  (The possibility of
code that is unsafe actually working does make testing difficult.)
