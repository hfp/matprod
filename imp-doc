MATPROD - A LIBRARY FOR MATRIX MULTIPLICATION WITH OPTIONAL PIPELINING
          Implementation Documentation 

Copyright (C) 2017, 2018 Radford M. Neal.

  The matprod library is free software; you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation; either version 2 of the License, or
  (at your option) any later version.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License along
  with this program; if not, write to the Free Software Foundation, Inc.,
  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.


The routines in the matprod library have been written to be reasonably
fast.  However, compared to the matrix multiply routines in optimized
BLAS libraries (DGEMM, DGEMV, and DDOT), the speed of the matprod
routines is limited by the requirement that they produce the same
results as the naive algorithms, by the requirement that no
substantial temporary storage be used, and by the lesser amount of
effort that has been expended in trying to make them fast.  The
general implementation strategy for matprod is described here, along
with some details of the implementation.


General algorithmic approaches.

This section describes the general algorithms used when computations
are done in a single thread, the matrices are small enough that the
computation is not split into parts to improve cache performance, and
special case code for small values of n or k does not apply.

The matprod_vec_vec procedure uses what is essentially the naive
algorithm, successively adding products of elements, since any
reordering of additions could change the result.

When matprod_mat_mat multiplies matrices x and y to produce result z,
it computes columns of z in succession.  To compute a column of z, the
column is first set to the sum of the products of the first two
columns of x and the first two elements of the column of y
corresponding that of z.  Then, the product of another column of x
with another element of y is added to this column of z - this being
done for two columns of x together (unless only one column remains),
in order to reduce accesses to elements of z.  Two columns of z are
computed concurrently by this method (unless only one remains) so that
the data fetched from the columns of x can be used twice.  When this
approach is applied to a small matrix, two columns of z will fit in
the L1 data cache, and so will be quickly accessible when products
with columns of x are added repeatedly to it (even if not all of x, y,
and z fit in the L1 cache).  Note that this should generally be faster
than the naive method of computing each element of the result as a dot
product of a row and column, since accesses to the elements of the row
will be non-sequential.

For matprod_outer, there is only one column of x, which is multiplied
by successive elements of y to give successive columns of z.  This is
presently done one column at a time.  (It is not clear that doing two
columns concurrently would be faster.)

The matprod_mat_vec procedure uses the same method as matprod_mat_mat,
with y having only a single column.

A different approach is used for matprod_vec_mat, for which columns of
x and z have only one element.  Successive elements of the result are
computed as dot products of x with successive columns of y.  To allow
use of SIMD instructions (or other instruction-level parallelism), and
to reuse data fetched from x, four such dot products are computed
concurrently (except when fewer than four columns of y remain).

The method for matprod_trans1 is similar to that for matprod_vec_mat,
except that the four dot products computed concurrently are for two
columns of x and two columns of y, which produce four elements of z in
a two-by-two block.

The method for matprod_trans2 is similar to that for matprod_mat_mat,
except that the elements multiplying columns of x are taken from rows
of y (rather than columns of y).

The implementation of matprod_trans12 is based on the fact that the
product of the transposes of two matrices is the transpose of the
product of these matrices with operands swapped.  The same procedures
used for matprod_mat_mat are used here, storing the result in a local
variable (on the stack), the transpose of which is then copied to z.
For large matrices, this must be done repeatedly for portions of z, in
order to limit the amount of stack space used to no more than about
16K bytes (both for cache performance reasons and because stack space
may be limited in some environments).


Optimizing for cache behaviour.

The code is written assuming that the processor has an L1 data cache
of 32K bytes (or more) and an L2 cache of 256K bytes (or more).  The
size of a cache line is assumed to be 64 bytes.  If these assumptions
are not true, the code will of course still work correctly, but
perhaps more slowly than it might with some other assumptions.  Note
that 4K 8-byte double values fit in the L1 data cache, but it is best
to rely only on somewhat fewer fitting, since the L1 cache will
contain some other values (eg, top entries in the stack) as well.

A last-level cache holding at least DOUBLES_IN_LLC doubles (currently
100000) is also assumed to possibly exist (though perhaps be the same
as the L2 cache).  If such a large last-level cache does not exist,
there is only a small penalty for using the code that assumes it does
exist.

For matprod_scalar_vec and matprod_vec_vec, there is no apparent way
of improving on the obvious method that accesses memory sequentially.

For matprod_vec_mat, if y has more than VEC_MAT_YROWS rows, the
operation is done on successive parts of the matrix, each consisting
of at most VEC_MAT_YROWS of the rows.  The second and later parts add
to the result found by earlier parts.  The value for VEC_MAT_YROWS,
currently 512, is designed to keep the vector x in the L1 cache, given
that x and four columns of y (all of length VEC_MAT_YROWS) are
accessed within the main loop.

For matprod_mat_vec, if x has more than MAT_VEC_XROWS rows, the
operation is done on successive parts of the matrix, each consisting
of at most MAT_VEC_XROWS of the rows.  The definition of
MAT_VEC_XROWS, currently 1024, is designed to keep the vector z in the
L1 cache, given that z and two columns of x (all of length
MAT_VEC_XROWS) are accessed within the main loop.

For matprod_outer, if x has more than OUTER_ROWS elements (and hence z
has more than OUTER_ROWS rows), the operation is done on successive
parts of x, each consisting of at most OUTER_ROWS elements.  The
definition of OUTER_ROWS, currently 2048-128, is designed to keep that
part of x in the L1 cache, given that it and the corresponding part of
a column of z are accessed within the main loop.

For matprod_mat_mat, if x has more than MAT_MAT_XROWS rows, the
operation is done on successive parts of the matrix, each consisting
of at most MAT_MAT_XROWS of the rows.  Furthermore, if x has more than
MAT_MAT_XCOLS, operations on each set of up to MAT_MAT_XROWS are split
into operations looking at most MAT_MAT_XCOLS columns of x, with all
except for the first operation adding to the sum from the previous
columns.

The definition of MAT_MAT_XROWS is designed to keep two columns of z
in the L1 cache, given that two columns of z and two columns of x (all
of length MAT_MAT_XROWS) are accessed within the outer loop over
columns of y and z.

The definition of MAT_MAT_XCOLS is designed to keep the submatrix of x
with up to MAT_MAT_XROWS and MAT_MAT_XCOLS in the L2 cache, while it
is multiplied repeatedly by columns of y.

If y is large, and rows of x will be split, matprod_mat_mat does
columns of y in groups, whose size is designed to keep these columns
of y and z in a last-level cache that can hold DOUBLES_IN_LLC doubles.


Special cases.

Special code is used to handle some cases in which n, k, or m is small
(for multiplication of an n x k matrix by a k x m matrix).  This can
reduce overhead when what would otherwise be the innermost loop would
be done only a small number of times.  When columns have only two
elements, it also allows AVX instuctions to operate on all elements in
two columns at once.

The vector-vector, matrix-vector, vector-matrix, and outer product
procedure are particular cases of such special-case optimization that
are made visible in the API, but other special cases are handled by
non-visible functions (including special cases of the matrix-vector,
vector-matrix, and outer product operations).


Avoidance of overflow.

The numbers of rows and columns in a matrix have the C "int" type,
which is typically 32 bits in size, allowing a maximum of 2^31-1 for
the number of rows or columns.  However, the number of elements in a
matrix (the product of the number of rows and the number of columns)
may be greater than 2^32-1.  Care is therefore required to avoid
overflow when accessing an element.

If an index into a matrix is computed from a row and column index, it
is necessary to cast the "int" values to "size_t" values - for
example, if x is a matrix with n rows, and i and j are row and column
indexes, element i,j should be accessed as x[i+(size_t)j*n], not as
x[i+j*n].  This form of access is usually avoided, however, in favour
of accesses using pointers into the matrix.  For example, if p points
to the first element in a column of x, then p[i] can be used to access
the element of that column in row i.  The element in row i of the next
column can be accessed as (p+n)[i], which should not be rewritten as
p[n+i], since n+i might overflow.

Note also that p += 2*n is not safe, but p += n; p += n; will be safe.
Similarly, p + 2*n is unsafe, but p + n + n, which means the same as
(p + n) + n, is safe.

Since the result of overflow with signed arithmetic is undefined in C,
it is possible that the unsafe code will actually work when overflow
happens.  But of course this cannot be relied on.  (The possibility of
code that is unsafe actually working does make testing difficult.)


SIMD instructions and alignment.

For Intel/AMD processors, the matprod routines can be compiled so that
some code sections use the SSE2, SSE3, AVX, and AVX2 instrinsics
provided by gcc.  Other compilers may also provide such intrinsics;
their availability is assumed to be given by whether the symbols
__SSE2__, __SSE3__, __AVX__, or __AVX2__ are defined.  For all
operations, code written in standard C is provided, for use on
non-Intel/AMD processors, or when the intrinsics are not available, or
when use of the intrinsics is disabled.  There are often code sections
for more than one of __SSE2__, __SSE3__, etc, with the most
powerful/recent option used when the routine is compiled for a
processor for which it is available.  Sometimes only SSE2 or SSE3 code
is provided, because there is no advantage to using AVX, or only AVX
code is provided, because trying to do it with SSE2 or SSE3 does not
work well.  (Or, of course, suitable AVX or SSE2/SSE3 code may just
not have been written yet.)

Optional specification of alignment of arguments is allowed (ie, of
the element of the matrix in the first row and column).  Both the
alignment boundary (8, 16, or 32 bytes) and a possible offset from
this boundary (a multiple of 8) may be specified.  

Some code may be generated conditionally on the alignment, for example
to do some operations before a loop so that the loop operations are
aligned at good boundaries.  For gcc and gcc look-alikes, alignment is
sometimes declared using the __builtin_assume_aligned function
provided in gcc, which may allow the compiler to generate better code.
In particular, gcc will convert some SSE or AVX intrinsics that
specify unaligned accesses to ones specifying aligned acccesses if it
knows the operand always aligned.  Nevertheless, selection of aligned
versus unaligned operations is often done explicitly in the code (via
some macros that are defined, such as _mm_loadA_pd).

Sometimes, rows or columns are stepped through by multiples of 4,
since this preserves whatever aligment (up to 32 bytes) and offset
were present for the first element of the matrix.

In some places, alignment is checked at run-time - for example, when
aligment of the start of a matrix column depends on whether the number
of rows in the matrix is even or odd.  

It is intended that even if no explicit SIMD instructions are used,
and no alignment information is specified, the matprod routines should
still be substantially faster than a naive implementation.
