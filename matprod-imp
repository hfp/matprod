MATPROD - A LIBRARY FOR MATRIX MULTIPLICATION WITH OPTIONAL PIPELINING
          Implementation Documentation 

Copyright (C) 2017 Radford M. Neal.

  The matprod library is free software; you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation; either version 2 of the License, or
  (at your option) any later version.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License along
  with this program; if not, write to the Free Software Foundation, Inc.,
  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.


The routines in the matprod library have been written to be reasonably
fast.  However, compared to the matrix multiply routines in optimized
BLAS libraries (DGEMM, DGEMV, and DDOT), the speed of the matprod
routines is limited by the requirement that they produce the same
results as the naive algorithms, and by the lesser amount of effort
that has been expended in trying to make them fast.  The general
implementation strategy for matprod is described here.


SIMD instructions and alignment.

For Intel/AMD processors, the matprod routines can be compiled so that
some code sections use the SSE2, SSE3, and AVX instrinsics provided by
gcc.  Other compilers may also provide such intrinsics; they will be
assumed available if the symbols __SSE2__, __SSE3__, or __AVX__ are
defined.  For all operations, code written in standard C is provided,
for use on non-Intel/AMD processors, or when the intrinsics are not
available, or when use of the intrinsics is disabled.  There are often
code sections for both AVX and SSE2/SSE3, with the latter used when
the routine is compiled for a processor with SSE2/SSE3 instructions
but not AVX instructions.  Sometimes only SSE2 or SSE3 code is
provided, because there is no advantage to using AVX, or only AVX code
is provided, because trying to do it with SSE2 or SSE3 does not work
well.  (Or, of course, suitable AVX or SSE2/SSE3 code may just not
have been written yet.)

Optional specification of alignment of arguments is allowed.  Some
code may be generated conditionally on the alignment, for example to
do some operations before a loop so that the loop operations are
aligned at good boundaries.  For gcc and gcc look-alikes, alignment is
sometimes declared using the __builtin_assume_aligned function
provided in gcc, which may allow the compiler to generate better code.
In particular, gcc will convert some SSE or AVX intrinsics that
specify unaligned accesses to ones specifying aligned acccesses if it
knows the operand always aligned.  Nevertheless, selection of aligned
versus unaligned operations is often done explicitly in the code (via
some macro definitons, such as for _mm_loadA_pd).

In some places, alignment is checked at run-time - for example, when
aligment of the start of a matrix column depends on whether the number
of rows in the matrix is even or odd.

It is intended that even if no explicit SIMD instructions are used,
and no alignment information is specified, the matprod routines should
still be substantially faster than a naive implementation.


Special cases.

Special code is used to handle some cases in which n, k, or m is small
(for multiplication of an n x k matrix by a k x m matrix).  This can
reduce overhead when what would otherwise be the innermost loop would
be done only a small number of times.  When columns have only two
elements, it also allows AVX instuctions to operate on all elements in
two columns at once.

The vector-vector, matrix-vector, vector-matrix, and outer product
procedure are particular cases of such special-case optimization that
are made visible in the API, but other special cases are handled by
non-visible functions.
