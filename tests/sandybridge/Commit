gcc-7 -g -std=gnu99 -O3 -ffp-contract=off -mavx -mtune=ivybridge
 
commit 8b6ee30aee7b39f454437db3d5a2ba255e2fb43e
Author: Radford Neal <radfordneal@gmail.com>
Date:   Tue Jan 2 15:44:04 2018 -0500

    Tweaks to matprod_vec_mat_k2
diff --git a/matprod.c b/matprod.c
index b073ed0..47c4016 100644
--- a/matprod.c
+++ b/matprod.c
@@ -1113,43 +1113,57 @@ static void matprod_vec_mat_k2 (double * MATPROD_RESTRICT x,
 
     int i = 0;
 
-#   if CAN_USE_SSE3 || CAN_USE_AVX
+#   if CAN_USE_SSE2 && (ALIGN_OFFSET & 8) || CAN_USE_SSE3 || CAN_USE_AVX
     {
-#       if CAN_USE_AVX
-            __m256d T = _mm256_set_pd (x[1], x[0], x[1], x[0]);
-#       else  /* CAN_USE_SSE3 */
-            __m128d T = _mm_set_pd (x[1], x[0]);
+#       if CAN_USE_AVX && (ALIGN_OFFSET & 8) == 0  /* AVX slower if unaligned */
+            __m256d TT = _mm256_set_pd (x[1], x[0], x[1], x[0]);
+#       endif
+
+        __m128d T = _mm_set_pd (x[1], x[0]);
+
+#       if ALIGN_OFFSET & 8
+            __m128d R = _mm_set_pd (x[0], x[1]);
 #       endif
 
 #       if ALIGN_FORWARD & 8
-        {
-            __m128d A = _mm_mul_pd (cast128(T), _mm_loadAA_pd(y));
-            _mm_store_sd (z+i, _mm_hadd_pd(A,A));
+            _mm_store_sd (z+i, _mm_add_sd (_mm_mul_sd (T, _mm_load_sd(y)),
+                                           _mm_mul_sd (R, _mm_load_sd(y+1))));
             y += 2;
             i += 1;
-        }
 #       endif
 
         while (i <= m-4) {
-#           if CAN_USE_AVX
+#           if CAN_USE_AVX && (ALIGN_OFFSET & 8) == 0  /* slower if unaligned */
             {
-                 __m256d A = _mm256_mul_pd (T, _mm256_loadAA_pd(y));
-                 __m256d B = _mm256_mul_pd (T, _mm256_loadAA_pd(y+4));
-                 _mm256_storeAA_pd(z+i,_mm256_hadd_pd
-                                    (_mm256_permute2f128_pd(A,B,0x20),
-                                     _mm256_permute2f128_pd(A,B,0x31)));
+                 __m256d A = _mm256_mul_pd (TT, _mm256_loadAA_pd(y));
+                 __m256d B = _mm256_mul_pd (TT, _mm256_loadAA_pd(y+4));
+                 _mm256_storeAA_pd (z+i,_mm256_hadd_pd
+                                          (_mm256_permute2f128_pd(A,B,0x20),
+                                           _mm256_permute2f128_pd(A,B,0x31)));
                 y += 8;
                 i += 4;
             }
-#           else  /* CAN_USE_SSE3 */
+#           elif ALIGN_OFFSET & 8  /* && CAN_USE_SSE2 */
+            {
+                 __m128d A, B;
+                 A = _mm_mul_pd (R, _mm_loadA_pd(y+1));
+                 B = _mm_mul_pd (T,_mm_loadh_pd (_mm_load_sd(y), y+3));
+                 _mm_storeA_pd (z+i, _mm_add_pd(A,B));
+                 A = _mm_mul_pd (R, _mm_loadA_pd(y+5));
+                 B = _mm_mul_pd (T,_mm_loadh_pd (_mm_load_sd(y+4), y+7));
+                 _mm_storeA_pd (z+i+2, _mm_add_pd(A,B));
+                y += 8;
+                i += 4;
+            }
+#           else /* (ALIGN_OFFSET & 8) == 0 && CAN_USE_SSE3 */
             {
                  __m128d A, B;
-                 A = _mm_mul_pd (T, _mm_loadAA_pd(y));
-                 B = _mm_mul_pd (T, _mm_loadAA_pd(y+2));
+                 A = _mm_mul_pd (T, _mm_loadA_pd(y));
+                 B = _mm_mul_pd (T, _mm_loadA_pd(y+2));
                  _mm_storeA_pd (z+i, _mm_hadd_pd(A,B));
-                 A = _mm_mul_pd (T, _mm_loadAA_pd(y+4));
-                 B = _mm_mul_pd (T, _mm_loadAA_pd(y+6));
-                 _mm_storeA_pd (z+2, _mm_hadd_pd(A,B));
+                 A = _mm_mul_pd (T, _mm_loadA_pd(y+4));
+                 B = _mm_mul_pd (T, _mm_loadA_pd(y+6));
+                 _mm_storeA_pd (z+i+2, _mm_hadd_pd(A,B));
                 y += 8;
                 i += 4;
             }
@@ -1157,16 +1171,29 @@ static void matprod_vec_mat_k2 (double * MATPROD_RESTRICT x,
         }
 
         if (i <= m-2) {
-             __m128d A = _mm_mul_pd (cast128(T), _mm_loadAA_pd(y));
-             __m128d B = _mm_mul_pd (cast128(T), _mm_loadAA_pd(y+2));
-             _mm_storeA_pd (z+i, _mm_hadd_pd(A,B));
+#           if ALIGN_OFFSET & 8
+                 __m128d A, B;
+                 A = _mm_mul_pd (R, _mm_loadA_pd(y+1));
+                 B = _mm_mul_pd (T, _mm_loadh_pd (_mm_load_sd(y), y+3));
+                 _mm_storeA_pd (z+i, _mm_add_pd(A,B));
+#           else
+                __m128d A, B;
+                A = _mm_mul_pd (T, _mm_loadAA_pd(y));
+                B = _mm_mul_pd (T, _mm_loadAA_pd(y+2));
+                _mm_storeA_pd (z+i, _mm_hadd_pd(A,B));
+#           endif
             y += 4;
             i += 2;
         }
 
         if (i < m) {
-            __m128d A = _mm_mul_pd (cast128(T), _mm_loadAA_pd(y));
-            _mm_store_sd (z+i, _mm_hadd_pd(A,A));
+#           if ALIGN_OFFSET & 8
+                _mm_store_sd (z+i,_mm_add_sd(_mm_mul_sd (T, _mm_load_sd(y)),
+                                             _mm_mul_sd (R, _mm_load_sd(y+1))));
+#           else
+                __m128d A = _mm_mul_pd (T, _mm_loadAA_pd(y));
+                _mm_store_sd (z+i, _mm_hadd_pd(A,A));
+#           endif
         }
     }
 
